{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a81b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym numpy networkx stable-baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "940d9f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1740 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1250        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003399002 |\n",
      "|    clip_fraction        | 0.00205     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.86       |\n",
      "|    explained_variance   | -3.7e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.61e+05    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.006      |\n",
      "|    value_loss           | 5.29e+05    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Initialize a PPO model with an MLP policy and the custom environment\u001b[39;00m\n\u001b[1;32m     85\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 86\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:281\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mrecord(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime/total_timesteps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps, exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdump(step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps)\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:215\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m advantages \u001b[38;5;241m=\u001b[39m rollout_data\u001b[38;5;241m.\u001b[39madvantages\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# Normalization does not make sense if mini batchsize == 1, see GH issue #325\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_advantage \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43madvantages\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    216\u001b[0m     advantages \u001b[38;5;241m=\u001b[39m (advantages \u001b[38;5;241m-\u001b[39m advantages\u001b[38;5;241m.\u001b[39mmean()) \u001b[38;5;241m/\u001b[39m (advantages\u001b[38;5;241m.\u001b[39mstd() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# ratio between old and new policy, should be one at the first iteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py:904\u001b[0m, in \u001b[0;36mTensor.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[1;32m    896\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    897\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing len to get tensor shape might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    898\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommended usage would be tensor.shape[0]. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    902\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    903\u001b[0m     )\n\u001b[0;32m--> 904\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "# Create a directed graph G using NetworkX to represent the supply chain\n",
    "G = nx.DiGraph()\n",
    "G.add_node('Supplier') # Add a Supplier node\n",
    "G.add_node('Retailer') # Add a Retailer node\n",
    "G.add_edge('Supplier', 'Retailer', weight=0)\n",
    " # Add an edge from the Supplier to the Retailer with an initial weight of 0\n",
    "\n",
    "# Define an Agent class\n",
    "class Agent:\n",
    "    def __init__(self, name):\n",
    "        self.name = name # Name of the agent\n",
    "\n",
    "    # Define a random action the agent can take\n",
    "    def act(self, state):\n",
    "        return random.randint(0, 50) # Random integer between 0 and 50\n",
    "\n",
    "# Define a custom Gym Environment\n",
    "class SupplyChainEnv(gym.Env):\n",
    "    def __init__(self, G, agents):\n",
    "        super(SupplyChainEnv, self).__init__()\n",
    "        self.G = G # Supply chain graph\n",
    "        self.agents = agents # Agents in the supply chain\n",
    "        \n",
    "        # Define the action space as multi-discrete, where each agent can perform an action from 0 to 50\n",
    "        self.action_space = spaces.MultiDiscrete([51 for _ in range(len(self.agents))])\n",
    "\n",
    "        # Define the observation space as a box from 0 to 50 with size equal to the number of nodes in the graph\n",
    "        self.observation_space = spaces.Box(low=0, high=50, shape=(len(self.G.nodes),))\n",
    "\n",
    "        self.reset() # Reset the environment\n",
    "\n",
    "    # Define the step function which will execute the agent's action and return the new state, reward, done status, and extra info\n",
    "    def step(self, action):\n",
    "        total_reward = 0 # Initialize total reward\n",
    "        \n",
    "        # Update graph based on actions\n",
    "        for i, (node, agent) in enumerate(self.agents.items()):\n",
    "            if self.G.in_edges(node): # If the node has incoming edges\n",
    "                supplier, _ = list(self.G.in_edges(node))[0] # Get the supplier for the node\n",
    "                self.G[supplier][node]['weight'] += action[i] # Update the weight of the edge based on the action\n",
    "\n",
    "        # Update inventory and calculate rewards\n",
    "        total_reward = 0\n",
    "        for node in self.G.nodes:\n",
    "            if self.G.in_edges(node):  # If the node has incoming edges\n",
    "                supplier, _ = list(self.G.in_edges(node))[0] # Get the supplier for the node\n",
    "                shipment = self.G[supplier][node]['weight']  # Get the shipment from the supplier\n",
    "                self.G.nodes[node]['inventory'] += shipment  # Add the shipment to the inventory\n",
    "                self.G[supplier][node]['weight'] = 0 # Reset the weight of the edge\n",
    "\n",
    "            # Calculate demand, sales, and update inventory\n",
    "            demand = self.G.nodes[node]['demand']\n",
    "            sales = min(demand, self.G.nodes[node]['inventory'])\n",
    "            self.G.nodes[node]['inventory'] -= sales\n",
    "\n",
    "            # Calculate reward as the negative absolute difference between demand and sales\n",
    "            reward = -abs(demand - sales)\n",
    "            total_reward += reward # Add to the total reward\n",
    "\n",
    "        return np.array([self.G.nodes[node]['inventory'] for node in self.G.nodes]).astype(float), total_reward, False, {}\n",
    "\n",
    "    # Define the reset function to reset the environment to its initial state\n",
    "    def reset(self):\n",
    "        for node in self.G.nodes:\n",
    "            self.G.nodes[node]['inventory'] = 50 # Reset inventory to 50\n",
    "            self.G.nodes[node]['demand'] = random.randint(0, 50) # Set a random demand between 0 and 50\n",
    "        return np.array([self.G.nodes[node]['inventory'] for node in self.G.nodes]).astype(float) # Return the initial state\n",
    "\n",
    "# Create agents\n",
    "agents = {node: Agent(node) for node in G.nodes}  # Create an agent for each node in the graph\n",
    "\n",
    "# Create environment\n",
    "env = SupplyChainEnv(G, agents)  # Create the supply chain environment with the graph and agents\n",
    "\n",
    "# Import the PPO algorithm from Stable Baselines 3\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Initialize a PPO model with an MLP policy and the custom environment\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
