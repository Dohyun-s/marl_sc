{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shimmy>=0.2.1\n",
      "  Obtaining dependency information for shimmy>=0.2.1 from https://files.pythonhosted.org/packages/d2/9f/d476f31b122aa65324f52cad67527b802bbb8d1f4660a0f06869d471433d/Shimmy-1.2.1-py3-none-any.whl.metadata\n",
      "  Downloading Shimmy-1.2.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/dohyun-s/opt/miniconda3/envs/camrl/lib/python3.11/site-packages (from shimmy>=0.2.1) (1.25.2)\n",
      "Requirement already satisfied: gymnasium>=0.27.0 in /Users/dohyun-s/opt/miniconda3/envs/camrl/lib/python3.11/site-packages (from shimmy>=0.2.1) (0.28.1)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in /Users/dohyun-s/opt/miniconda3/envs/camrl/lib/python3.11/site-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/dohyun-s/opt/miniconda3/envs/camrl/lib/python3.11/site-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/dohyun-s/opt/miniconda3/envs/camrl/lib/python3.11/site-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (4.7.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/dohyun-s/opt/miniconda3/envs/camrl/lib/python3.11/site-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (0.0.4)\n",
      "Downloading Shimmy-1.2.1-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: shimmy\n",
      "Successfully installed shimmy-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install 'shimmy>=0.2.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 10:41:34,953\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-08-14 10:41:35,594\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from inventory_management import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'InvManagement-v1'\n",
    "\n",
    "env_config = {'I0': [100, 120, 150, 150],\n",
    "              'p': 4.1,\n",
    "              'dist': 1,\n",
    "              'dist_param': {'mu': 20},\n",
    "              'seed_int': 0,\n",
    "              'max_rewards': 1000,\n",
    "              'backlog': False,\n",
    "              'r': [1.0, 1.0, 0.75, 0.5, 0.5],\n",
    "              'k': [0.10, 0.075, 0.05, 0.025, 0.025],\n",
    "              'h': [0.10, 0.10, 0.05, 0.05],\n",
    "              'c':[120, 100, 100, 90],\n",
    "              'L': [2, 2, 3, 3],\n",
    "              'periods': 30}\n",
    "\n",
    "def register_env(env_name, env_config={}):\n",
    "    env = create_env(env_name)\n",
    "    return env\n",
    "env = register_env('InvManagement-v1', env_config=env_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env  = InvManagementMasterEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -580     |\n",
      "| time/              |          |\n",
      "|    fps             | 2580     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -574         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1795         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052642846 |\n",
      "|    clip_fraction        | 0.0485       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.25        |\n",
      "|    explained_variance   | -0.000132    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.81e+04     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00405     |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 5.59e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -580         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1631         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043163644 |\n",
      "|    clip_fraction        | 0.035        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.24        |\n",
      "|    explained_variance   | 3.2e-05      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.61e+04     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00289     |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 5.39e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -575         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1574         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044297054 |\n",
      "|    clip_fraction        | 0.0233       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.23        |\n",
      "|    explained_variance   | 4.89e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.63e+04     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00233     |\n",
      "|    std                  | 0.986        |\n",
      "|    value_loss           | 5.47e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -573         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1540         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034387622 |\n",
      "|    clip_fraction        | 0.0339       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.22        |\n",
      "|    explained_variance   | 7.75e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.45e+04     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00282     |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 5.07e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -574        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1518        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004683161 |\n",
      "|    clip_fraction        | 0.0437      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.22       |\n",
      "|    explained_variance   | 2.98e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.49e+04    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00306    |\n",
      "|    std                  | 0.989       |\n",
      "|    value_loss           | 4.99e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -574        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1503        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004735615 |\n",
      "|    clip_fraction        | 0.0278      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.22       |\n",
      "|    explained_variance   | 8.34e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.49e+04    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.002      |\n",
      "|    std                  | 0.986       |\n",
      "|    value_loss           | 4.84e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -576         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1493         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035972702 |\n",
      "|    clip_fraction        | 0.0387       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.21        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.41e+04     |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0027      |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 4.74e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -578         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1485         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047018053 |\n",
      "|    clip_fraction        | 0.0497       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.2         |\n",
      "|    explained_variance   | 2.98e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.47e+04     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00448     |\n",
      "|    std                  | 0.978        |\n",
      "|    value_loss           | 4.74e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -580         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1479         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057705888 |\n",
      "|    clip_fraction        | 0.048        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.19        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.98e+04     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00426     |\n",
      "|    std                  | 0.978        |\n",
      "|    value_loss           | 4.6e+04      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -570         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1474         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032823463 |\n",
      "|    clip_fraction        | 0.0332       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.18        |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.15e+04     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00258     |\n",
      "|    std                  | 0.972        |\n",
      "|    value_loss           | 4.53e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -570         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1469         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032273277 |\n",
      "|    clip_fraction        | 0.0239       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.16        |\n",
      "|    explained_variance   | 2.98e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.89e+04     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.002       |\n",
      "|    std                  | 0.967        |\n",
      "|    value_loss           | 4.21e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -579         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1466         |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041204165 |\n",
      "|    clip_fraction        | 0.0394       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.15        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2e+04        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00308     |\n",
      "|    std                  | 0.963        |\n",
      "|    value_loss           | 4.16e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -573         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1463         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052338736 |\n",
      "|    clip_fraction        | 0.0606       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.16        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.12e+04     |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00467     |\n",
      "|    std                  | 0.973        |\n",
      "|    value_loss           | 4.24e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -573         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1459         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023194682 |\n",
      "|    clip_fraction        | 0.0294       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.18        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.8e+04      |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00191     |\n",
      "|    std                  | 0.974        |\n",
      "|    value_loss           | 4e+04        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -574         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1454         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038033137 |\n",
      "|    clip_fraction        | 0.0317       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.18        |\n",
      "|    explained_variance   | 2.98e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.99e+04     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00174     |\n",
      "|    std                  | 0.978        |\n",
      "|    value_loss           | 3.93e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -577         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1452         |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033127868 |\n",
      "|    clip_fraction        | 0.0233       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.17        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.14e+04     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00177     |\n",
      "|    std                  | 0.966        |\n",
      "|    value_loss           | 3.81e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -574         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1449         |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071720704 |\n",
      "|    clip_fraction        | 0.0618       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.12        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.95e+04     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0053      |\n",
      "|    std                  | 0.951        |\n",
      "|    value_loss           | 3.76e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -566        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1444        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005593574 |\n",
      "|    clip_fraction        | 0.0305      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.09       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.56e+04    |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00125    |\n",
      "|    std                  | 0.942       |\n",
      "|    value_loss           | 3.6e+04     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -572         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1440         |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 28           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044869264 |\n",
      "|    clip_fraction        | 0.0569       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.08        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.6e+04      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0035      |\n",
      "|    std                  | 0.943        |\n",
      "|    value_loss           | 3.42e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -571         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1437         |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041699614 |\n",
      "|    clip_fraction        | 0.0496       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.08        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.43e+04     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00358     |\n",
      "|    std                  | 0.945        |\n",
      "|    value_loss           | 3.47e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -566        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1434        |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005017299 |\n",
      "|    clip_fraction        | 0.05        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.08       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.63e+04    |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00322    |\n",
      "|    std                  | 0.942       |\n",
      "|    value_loss           | 3.31e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -568        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1432        |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004165996 |\n",
      "|    clip_fraction        | 0.0162      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.08       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.54e+04    |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.000619   |\n",
      "|    std                  | 0.945       |\n",
      "|    value_loss           | 3.26e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -566         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1431         |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 34           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034370595 |\n",
      "|    clip_fraction        | 0.0155       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.09        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.51e+04     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.000701    |\n",
      "|    std                  | 0.948        |\n",
      "|    value_loss           | 3.15e+04     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Initialize a PPO model with an MLP policy and the custom environment\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model \u001b[39m=\u001b[39m PPO(\u001b[39m\"\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m\"\u001b[39m, env, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m100000000\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/camrl/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    315\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/camrl/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:281\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mrecord(\u001b[39m\"\u001b[39m\u001b[39mtime/total_timesteps\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps, exclude\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtensorboard\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    279\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdump(step\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps)\n\u001b[0;32m--> 281\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    283\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[1;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/camrl/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:200\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m approx_kl_divs \u001b[39m=\u001b[39m []\n\u001b[1;32m    199\u001b[0m \u001b[39m# Do a complete pass on the rollout buffer\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[39mfor\u001b[39;00m rollout_data \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrollout_buffer\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size):\n\u001b[1;32m    201\u001b[0m     actions \u001b[39m=\u001b[39m rollout_data\u001b[39m.\u001b[39mactions\n\u001b[1;32m    202\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mDiscrete):\n\u001b[1;32m    203\u001b[0m         \u001b[39m# Convert discrete action from float to long\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/camrl/lib/python3.11/site-packages/stable_baselines3/common/buffers.py:477\u001b[0m, in \u001b[0;36mRolloutBuffer.get\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    475\u001b[0m start_idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    476\u001b[0m \u001b[39mwhile\u001b[39;00m start_idx \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_envs:\n\u001b[0;32m--> 477\u001b[0m     \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_samples(indices[start_idx : start_idx \u001b[39m+\u001b[39;49m batch_size])\n\u001b[1;32m    478\u001b[0m     start_idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_size\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/camrl/lib/python3.11/site-packages/stable_baselines3/common/buffers.py:488\u001b[0m, in \u001b[0;36mRolloutBuffer._get_samples\u001b[0;34m(self, batch_inds, env)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_samples\u001b[39m(\n\u001b[1;32m    481\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    482\u001b[0m     batch_inds: np\u001b[39m.\u001b[39mndarray,\n\u001b[1;32m    483\u001b[0m     env: Optional[VecNormalize] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    484\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RolloutBufferSamples:  \u001b[39m# type: ignore[signature-mismatch] #FIXME\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     data \u001b[39m=\u001b[39m (\n\u001b[1;32m    486\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservations[batch_inds],\n\u001b[1;32m    487\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactions[batch_inds],\n\u001b[0;32m--> 488\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues[batch_inds]\u001b[39m.\u001b[39mflatten(),\n\u001b[1;32m    489\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_probs[batch_inds]\u001b[39m.\u001b[39mflatten(),\n\u001b[1;32m    490\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvantages[batch_inds]\u001b[39m.\u001b[39mflatten(),\n\u001b[1;32m    491\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturns[batch_inds]\u001b[39m.\u001b[39mflatten(),\n\u001b[1;32m    492\u001b[0m     )\n\u001b[1;32m    493\u001b[0m     \u001b[39mreturn\u001b[39;00m RolloutBufferSamples(\u001b[39m*\u001b[39m\u001b[39mtuple\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_torch, data)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Initialize a PPO model with an MLP policy and the custom environment\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 - Reward: -575.9864072782568\n",
      "Episode 2 - Reward: -581.095255463167\n",
      "Episode 3 - Reward: -524.1667272154951\n",
      "Episode 4 - Reward: -610.9622701976109\n",
      "Episode 5 - Reward: -544.7831969218992\n",
      "Episode 6 - Reward: -601.3318658758786\n",
      "Episode 7 - Reward: -537.8596579971176\n",
      "Episode 8 - Reward: -553.2892424806655\n",
      "Episode 9 - Reward: -552.8713571364614\n",
      "Episode 10 - Reward: -587.5774421711185\n",
      "Episode 11 - Reward: -569.8622826465187\n",
      "Episode 12 - Reward: -576.4927704521631\n",
      "Episode 13 - Reward: -598.9077548068637\n",
      "Episode 14 - Reward: -612.2833771247272\n",
      "Episode 15 - Reward: -585.4125842425968\n",
      "Episode 16 - Reward: -561.065642053648\n",
      "Episode 17 - Reward: -564.1620553124748\n",
      "Episode 18 - Reward: -536.1948669278564\n",
      "Episode 19 - Reward: -524.790776559083\n",
      "Episode 20 - Reward: -559.9779814074915\n",
      "Episode 21 - Reward: -545.3907907494857\n",
      "Episode 22 - Reward: -536.4919052500214\n",
      "Episode 23 - Reward: -539.3532483435879\n",
      "Episode 24 - Reward: -593.8497391817965\n",
      "Episode 25 - Reward: -574.988968702493\n",
      "Episode 26 - Reward: -597.3580966736981\n",
      "Episode 27 - Reward: -574.3874573508726\n",
      "Episode 28 - Reward: -562.2812517490428\n",
      "Episode 29 - Reward: -524.9825940713555\n",
      "Episode 30 - Reward: -556.7402526117504\n",
      "Episode 31 - Reward: -570.2961881602681\n",
      "Episode 32 - Reward: -578.411693941386\n",
      "Episode 33 - Reward: -544.5625310003937\n",
      "Episode 34 - Reward: -566.8615380173433\n",
      "Episode 35 - Reward: -578.0199100414173\n",
      "Episode 36 - Reward: -573.6977193014567\n",
      "Episode 37 - Reward: -595.4420369912748\n",
      "Episode 38 - Reward: -569.1385874255302\n",
      "Episode 39 - Reward: -630.1469884980298\n",
      "Episode 40 - Reward: -566.7846128227742\n",
      "Episode 41 - Reward: -552.1476233036622\n",
      "Episode 42 - Reward: -571.3275685047836\n",
      "Episode 43 - Reward: -568.1851239498505\n",
      "Episode 44 - Reward: -530.0544050376715\n",
      "Episode 45 - Reward: -539.6706422526917\n",
      "Episode 46 - Reward: -572.3453806349507\n",
      "Episode 47 - Reward: -526.2832929705282\n",
      "Episode 48 - Reward: -533.2954207659565\n",
      "Episode 49 - Reward: -582.496266936656\n",
      "Episode 50 - Reward: -578.1316512537448\n",
      "Episode 51 - Reward: -592.8792887993421\n",
      "Episode 52 - Reward: -568.0859836866011\n",
      "Episode 53 - Reward: -563.1690099713998\n",
      "Episode 54 - Reward: -597.853829963763\n",
      "Episode 55 - Reward: -542.0488265764747\n",
      "Episode 56 - Reward: -593.2580545568702\n",
      "Episode 57 - Reward: -529.7648442645877\n",
      "Episode 58 - Reward: -573.9133151092135\n",
      "Episode 59 - Reward: -529.2728898479269\n",
      "Episode 60 - Reward: -572.2413351593998\n",
      "Episode 61 - Reward: -579.6932387137499\n",
      "Episode 62 - Reward: -604.7671850500877\n",
      "Episode 63 - Reward: -603.1985839162309\n",
      "Episode 64 - Reward: -554.7696130138654\n",
      "Episode 65 - Reward: -556.9940313560868\n",
      "Episode 66 - Reward: -539.7667620554594\n",
      "Episode 67 - Reward: -536.2117935022507\n",
      "Episode 68 - Reward: -556.9735438287088\n",
      "Episode 69 - Reward: -554.1638606987054\n",
      "Episode 70 - Reward: -536.2593179113861\n",
      "Episode 71 - Reward: -590.1622413177538\n",
      "Episode 72 - Reward: -584.8890758487456\n",
      "Episode 73 - Reward: -610.4490462029047\n",
      "Episode 74 - Reward: -555.6476325954278\n",
      "Episode 75 - Reward: -603.9788888142409\n",
      "Episode 76 - Reward: -566.7481946967463\n",
      "Episode 77 - Reward: -563.4066222716731\n",
      "Episode 78 - Reward: -596.4887511658808\n",
      "Episode 79 - Reward: -544.349240957104\n",
      "Episode 80 - Reward: -564.0921691965231\n",
      "Episode 81 - Reward: -570.9129540880638\n",
      "Episode 82 - Reward: -578.5207678673921\n",
      "Episode 83 - Reward: -549.537768161215\n",
      "Episode 84 - Reward: -580.4883652870011\n",
      "Episode 85 - Reward: -521.8746263545204\n",
      "Episode 86 - Reward: -535.4679046260436\n",
      "Episode 87 - Reward: -546.3620766574969\n",
      "Episode 88 - Reward: -580.1457373724134\n",
      "Episode 89 - Reward: -531.3016141989611\n",
      "Episode 90 - Reward: -555.6454546871666\n",
      "Episode 91 - Reward: -585.5448813303002\n",
      "Episode 92 - Reward: -559.9113770018321\n",
      "Episode 93 - Reward: -550.1011779105222\n",
      "Episode 94 - Reward: -562.1816208191606\n",
      "Episode 95 - Reward: -575.853928722048\n",
      "Episode 96 - Reward: -520.7370913787205\n",
      "Episode 97 - Reward: -570.6082798064853\n",
      "Episode 98 - Reward: -573.436425958199\n",
      "Episode 99 - Reward: -547.5831628922115\n",
      "Episode 100 - Reward: -619.0670257457072\n",
      "Average Reward: -565.7297641268011\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_episodes = 100\n",
    "episode_rewards = []\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    print(f\"Episode {i + 1} - Reward: {episode_reward}\")\n",
    "\n",
    "# Calculate and print average reward\n",
    "average_reward = sum(episode_rewards) / num_episodes\n",
    "print(f\"Average Reward: {average_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
