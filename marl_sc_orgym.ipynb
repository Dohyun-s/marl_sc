{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: unmatched '\n"
     ]
    }
   ],
   "source": [
    "!pip install 'shimmy>=0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 10:30:31,897\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from inventory_management import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'InvManagement-v1'\n",
    "\n",
    "env_config = {'I0': [100, 120, 150, 150],\n",
    "              'p': 4.1,\n",
    "              'dist': 1,\n",
    "              'dist_param': {'mu': 20},\n",
    "              'seed_int': 0,\n",
    "              'max_rewards': 1000,\n",
    "              'backlog': False,\n",
    "              'r': [1.0, 1.0, 0.75, 0.5, 0.5],\n",
    "              'k': [0.10, 0.075, 0.05, 0.025, 0.025],\n",
    "              'h': [0.10, 0.10, 0.05, 0.05],\n",
    "              'c':[120, 100, 100, 90],\n",
    "              'L': [2, 2, 3, 3],\n",
    "              'periods': 30}\n",
    "\n",
    "def register_env(env_name, env_config={}):\n",
    "    env = create_env(env_name)\n",
    "    return env\n",
    "env = register_env('InvManagement-v1', env_config=env_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env  = InvManagementMasterEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing shimmy installation. You an OpenAI Gym environment. Stable-Baselines3 (SB3) has transitioned to using Gymnasium internally. In order to use OpenAI Gym environments with SB3, you need to install shimmy (`pip install 'shimmy>=0.2.1'`).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/miniconda3/envs/camrl/lib/python3.11/site-packages/stable_baselines3/common/vec_env/patch_gym.py:40\u001b[0m, in \u001b[0;36m_patch_env\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mshimmy\u001b[39;00m  \u001b[39m# pytype: disable=import-error\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shimmy'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines3\u001b[39;00m \u001b[39mimport\u001b[39;00m PPO\n\u001b[1;32m      3\u001b[0m \u001b[39m# Initialize a PPO model with an MLP policy and the custom environment\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m PPO(\u001b[39m\"\u001b[39;49m\u001b[39mMlpPolicy\u001b[39;49m\u001b[39m\"\u001b[39;49m, env, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m model\u001b[39m.\u001b[39mlearn(total_timesteps\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/camrl/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:104\u001b[0m, in \u001b[0;36mPPO.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     79\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m     _init_setup_model: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    103\u001b[0m ):\n\u001b[0;32m--> 104\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    105\u001b[0m         policy,\n\u001b[1;32m    106\u001b[0m         env,\n\u001b[1;32m    107\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m    108\u001b[0m         n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[1;32m    109\u001b[0m         gamma\u001b[39m=\u001b[39;49mgamma,\n\u001b[1;32m    110\u001b[0m         gae_lambda\u001b[39m=\u001b[39;49mgae_lambda,\n\u001b[1;32m    111\u001b[0m         ent_coef\u001b[39m=\u001b[39;49ment_coef,\n\u001b[1;32m    112\u001b[0m         vf_coef\u001b[39m=\u001b[39;49mvf_coef,\n\u001b[1;32m    113\u001b[0m         max_grad_norm\u001b[39m=\u001b[39;49mmax_grad_norm,\n\u001b[1;32m    114\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[1;32m    115\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[1;32m    116\u001b[0m         stats_window_size\u001b[39m=\u001b[39;49mstats_window_size,\n\u001b[1;32m    117\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[1;32m    118\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[1;32m    119\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    120\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    121\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m    122\u001b[0m         _init_setup_model\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    123\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49m(\n\u001b[1;32m    124\u001b[0m             spaces\u001b[39m.\u001b[39;49mBox,\n\u001b[1;32m    125\u001b[0m             spaces\u001b[39m.\u001b[39;49mDiscrete,\n\u001b[1;32m    126\u001b[0m             spaces\u001b[39m.\u001b[39;49mMultiDiscrete,\n\u001b[1;32m    127\u001b[0m             spaces\u001b[39m.\u001b[39;49mMultiBinary,\n\u001b[1;32m    128\u001b[0m         ),\n\u001b[1;32m    129\u001b[0m     )\n\u001b[1;32m    131\u001b[0m     \u001b[39m# Sanity check, otherwise it will lead to noisy gradient and NaN\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[39m# because of the advantage normalization\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[39mif\u001b[39;00m normalize_advantage:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/camrl/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:81\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, stats_window_size, tensorboard_log, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     59\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     60\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     supported_action_spaces: Optional[Tuple[Type[spaces\u001b[39m.\u001b[39mSpace], \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     80\u001b[0m ):\n\u001b[0;32m---> 81\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     82\u001b[0m         policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[1;32m     83\u001b[0m         env\u001b[39m=\u001b[39;49menv,\n\u001b[1;32m     84\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m     85\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[1;32m     86\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m     87\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     88\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[1;32m     89\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[1;32m     90\u001b[0m         support_multi_env\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     91\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m     92\u001b[0m         stats_window_size\u001b[39m=\u001b[39;49mstats_window_size,\n\u001b[1;32m     93\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[1;32m     94\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49msupported_action_spaces,\n\u001b[1;32m     95\u001b[0m     )\n\u001b[1;32m     97\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_steps \u001b[39m=\u001b[39m n_steps\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m=\u001b[39m gamma\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/camrl/lib/python3.11/site-packages/stable_baselines3/common/base_class.py:169\u001b[0m, in \u001b[0;36mBaseAlgorithm.__init__\u001b[0;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m env \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     env \u001b[39m=\u001b[39m maybe_make_env(env, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 169\u001b[0m     env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrap_env(env, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, monitor_wrapper)\n\u001b[1;32m    171\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mobservation_space\n\u001b[1;32m    172\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/camrl/lib/python3.11/site-packages/stable_baselines3/common/base_class.py:216\u001b[0m, in \u001b[0;36mBaseAlgorithm._wrap_env\u001b[0;34m(env, verbose, monitor_wrapper)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" \"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39mWrap environment with the appropriate wrappers if needed.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39mFor instance, to have a vectorized environment\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39m:return: The wrapped environment.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(env, VecEnv):\n\u001b[1;32m    215\u001b[0m     \u001b[39m# Patch to support gym 0.21/0.26 and gymnasium\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     env \u001b[39m=\u001b[39m _patch_env(env)\n\u001b[1;32m    217\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_wrapped(env, Monitor) \u001b[39mand\u001b[39;00m monitor_wrapper:\n\u001b[1;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m verbose \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/camrl/lib/python3.11/site-packages/stable_baselines3/common/vec_env/patch_gym.py:42\u001b[0m, in \u001b[0;36m_patch_env\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mshimmy\u001b[39;00m  \u001b[39m# pytype: disable=import-error\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     43\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMissing shimmy installation. You an OpenAI Gym environment. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mStable-Baselines3 (SB3) has transitioned to using Gymnasium internally. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to use OpenAI Gym environments with SB3, you need to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minstall shimmy (`pip install \u001b[39m\u001b[39m'\u001b[39m\u001b[39mshimmy>=0.2.1\u001b[39m\u001b[39m'\u001b[39m\u001b[39m`).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m     49\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m     50\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mYou provided an OpenAI Gym environment. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mWe strongly recommend transitioning to Gymnasium environments. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mStable-Baselines3 is automatically wrapping your environments in a compatibility \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlayer, which could potentially cause issues.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     56\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m signature(env\u001b[39m.\u001b[39munwrapped\u001b[39m.\u001b[39mreset)\u001b[39m.\u001b[39mparameters:\n\u001b[1;32m     57\u001b[0m     \u001b[39m# Gym 0.26+ env\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing shimmy installation. You an OpenAI Gym environment. Stable-Baselines3 (SB3) has transitioned to using Gymnasium internally. In order to use OpenAI Gym environments with SB3, you need to install shimmy (`pip install 'shimmy>=0.2.1'`)."
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Initialize a PPO model with an MLP policy and the custom environment\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_config = dict(\n",
    "    env=env_name,\n",
    "    num_workers=2,\n",
    "    env_config=env_config,\n",
    "    model=dict(\n",
    "        vf_share_layers=False,\n",
    "        fcnet_activation='relu',\n",
    "        fcnet_hiddens=[512]\n",
    "    ),\n",
    "    lr=1e-3,\n",
    "    train_batch_size=4000\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 10:32:37,637\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "2023-08-14 10:32:37,756\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'agents' from 'ray.rllib' (/Users/dohyun-s/opt/miniconda3/envs/camrl/lib/python3.11/site-packages/ray/rllib/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mray\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m \u001b[39mimport\u001b[39;00m agents\n\u001b[1;32m      4\u001b[0m ray\u001b[39m.\u001b[39minit(ignore_reinit_error\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m agent \u001b[39m=\u001b[39m agents\u001b[39m.\u001b[39mppo\u001b[39m.\u001b[39mPPOTrainer(env\u001b[39m=\u001b[39menv_name, config\u001b[39m=\u001b[39mrl_config)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'agents' from 'ray.rllib' (/Users/dohyun-s/opt/miniconda3/envs/camrl/lib/python3.11/site-packages/ray/rllib/__init__.py)"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib import agents\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "agent = agents.ppo.PPOTrainer(env=env_name, config=rl_config)\n",
    "results = []\n",
    "for i in range(20):\n",
    "    res = agent.train()\n",
    "    if i %5 == 0:\n",
    "      print(i)\n",
    "    results.append(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
